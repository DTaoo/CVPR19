
<html>
<head>
<title>Listen to Image</title>
<meta name="google-site-verification" content="NksNPfO4SApMtvU2rGHxr4DPan2Uy6Pz-rP9cA0k1mg" />
<script src='https://code.jquery.com/jquery-2.2.0.min.js'></script>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-17813713-5', 'auto');
ga('create', 'UA-17813713-3', 'mit.edu');
ga('send', 'pageview');

</script>

<style>
@media screen and (max-device-width: 480px){
body{
-webkit-text-size-adjust: 100%;
}
}
body
{
font-family : Arial;
font-size : 16px;
background-color : #f2f2f2;
}
.content
{
width : 800px;
padding : 25px 25px;
margin : 25px auto;
background-color : #fff;
border-radius: 20px;
border : #e3e1e4 1px solid;
}
.content-title {
color : #000;
border : none;
margin-top : 25px;
padding-bottom : 0;
margin-bottom : 0;
background-color : inherit;
}

.content-names {
padding : 10px;
padding-top : 0;
border : none;
box-shadow : none;
background-color : inherit;
}

a, a:visited
{
color : #002a6d;
color : blue;
}

#authors
{
text-align : center;
}

#conference
{
text-align : center;
font-style : italic;
}

#authors span
{
margin : 0 10px;
}

h1
{
text-align : center;
font-family : Arial;
font-size : 40px;
}
h2 {
font-family : Arial;
font-size : 30px;
padding : 0; margin : 10px;
}
h3 {
font-family : Arial;
font-size : 20px;
padding : 0; margin : 10px;
}

p {
line-height : 130%;
margin : 10px;
}
pre {
line-height : 130%;
margin : 10px;
padding : 20px;
background-color : #2d2d2d;
color : white;
border-radius : 5px;
}

li {
margin : 10px 0;
}

.samples {
float : left;
width : 50%;
text-align : center;
}
.cond {
float : left;
margin : 0 40px;
}
.cond-container {
width : 700px;
margin : 0 auto;
text-align : center;
}
</style>
</head>

<body>

<div class="content content-title">
<h1>Listen to the Image</h1>


</div>

<div class="content content-names">

<p id="authors">
<span><a href="https://dtaoo.github.io">Di Hu</a> </span>
<span><a href="https://redwang.github.io/">Dong Wang</a> *
<span><a href="https://scholar.google.com.hk/citations?user=ahUibskAAAAJ&hl=zh-CN">Xuelong Li</a></span>
<span><a href="http://www.escience.cn/people/fpnie">Feiping Nie</a></span>
<span><a href="http://crabwq.github.io/">Qi Wang</a></span>
<br>
Northwestern Polytechnical University<br>
CVPR 2019<br>
</p>

</div>

<div class="content">


<div style="float:right; width :250px; text-align:justify;">
<a href="https://dtaoo.github.io/papers/2019_voice.pdf">
<img src="ims/paper.jpg" height="300px"><br>
<strong>Download Paper</strong>
</a>
</div>

<h2>Abstract</h2>

<p>
Visual-to-auditory sensory substitution devices can assist the blind in sensing the visual environment by translating
the visual information into a sound pattern. To improve the translation quality, the task performances of the blind are usually employed to evaluate different encoding schemes. <strong>In contrast to the toilsome human-based assessment, we argue that machine model can be also developed for evaluation, and more efficient</strong>. To this end, we firstly propose two distinct cross-modal perception model w.r.t. the late-blind and congenitally-blind cases, which aim to generate concrete visual contents based on the translated sound. To validate the functionality of proposed models, two novel optimization strategies w.r.t. the primary encoding scheme are presented. Further, we conduct sets of humanbased experiments to evaluate and compare them with the conducted machine-based assessments in the cross-modal generation task. Their highly consistent results w.r.t. different encoding schemes indicate that using machine model to accelerate optimization evaluation and reduce experimental cost is feasible to some extent, <strong>which could dramatically promote the upgrading of encoding scheme then help the blind to improve their visual perception ability.</strong>
</p>
<br clear="both">
</div>


<div class="content">
<h2>Sensory Substitution for the Blind</h2>

<div style='text-align:center;'><img src='ims/blind.jpg' width='500'></div>

<p>There are millions of blind people all over the world, how to help them to “re-see” the outside world is a significant but challenging task. According to the theory of cross-modal plasticity, it becomes possible to use other organs (e.g., ears) as the sensor to “visually” perceive the environment. Hence, in the past decades, there have been several projects attempting to help the disabled to recover their lost senses via other sensory channels, and the relevant equipments are usually named as <strong>Sensory Substitution (SS)</strong> devices. And the well-deployed one is the visual-to-auditory SS device of vOICe. After 10-15 hours of training, <strong>the regions of visual cortex become active</strong> due to cross-modal plasticity.
 </p>

</div>


<div class="content">
<h2>Hearing the Hidden Representation</h2>

<p>
Although the network is trained without ground truth labels, it learns rich sound features. We visualize these features by finding sounds that maximally activate a particular hidden unit.
Click the images below to hear what sounds activate that unit. <strong style='color:red;'>Turn on your speakers!</strong> You will hear the top 9 sounds that activate that unit.</p>

<h3>Visualizing conv7</h3>

<p>We visualize units in the deep layers in the network from <tt>conv7</tt>. Since we are deep in the network, sound detectors for high-level concepts can emerge automatically. Note the images are shown only for visualization purposes, and not used in analyzing the sounds.</p>


<script>$(document).ready(function() {
$('.snd-cnt').click(function(e){
$('.snd').removeClass('playing');
$(this).children('.snd').addClass('playing');
$('audio').remove();
$('body').append('<audio><source src="' + $(this).children('.snd').data('sound') +'" type="audio/mpeg"></audio>');
e.stopPropagation();
$("audio").bind("ended", function() { $(".snd").removeClass("playing"); }).get(0).play();

$("video").each(function() { this.pause(); this.currentTime = 0; });
});
$(document).click(function() {
$("audio").remove(); $('.snd').removeClass('playing');
});
});</script>
<style>
img.snd { cursor : pointer; border : 10px solid white; margin-bottom : 5px; border-radius : 25px; opacity:1;}
.snd-cnt:hover .snd { border : 10px solid red !important; }
.playing { border : 10px solid red !important; }
.snd-cnt { text-align:center; width:168px; display:inline-block; margin : 10px; cursor : pointer;}
.snd-small { width : 130px; }
.snd-tiny { width : 90px; margin : 2px; }
.mini-speaker {
position : relative;
top : 3px;
}
</style>
<div style='text-align:center;'>
<div class='snd-cnt'><img class=snd src='0128.jpg' data-sound=0128.mp3 width=158 height=158><br><img src='speaker.png' class='mini-speaker' width=20 height=20> Motor-like</div>
<div class='snd-cnt'><img class=snd src='0126.jpg' data-sound=0126.mp3 width=158 height=158><br><img src='speaker.png' class='mini-speaker' width=20 height=20> Dog-like</div>
<div class='snd-cnt'><img class=snd src='0659.jpg' data-sound=0659.mp3 width=158 height=158><br><img src='speaker.png' class='mini-speaker' width=20 height=20> Bird-like</div>
<div class='snd-cnt'><img class=snd src='0037.jpg' data-sound=0037.mp3 width=158 height=158><br><img src='speaker.png' class='mini-speaker' width=20 height=20> Sports Chatter-like</div>
<div class='snd-cnt'><img class=snd src='0113.jpg' data-sound=0113.mp3 width=158 height=158><br><img src='speaker.png' class='mini-speaker' width=20 height=20> Music-like</div>
<div class='snd-cnt'><img class=snd src='0350.jpg' data-sound=0350.mp3 width=158 height=158><br><img src='speaker.png' class='mini-speaker' width=20 height=20> Marching Band-like</div>
<div class='snd-cnt'><img class=snd src='0137.jpg' data-sound=0137.mp3 width=158 height=158><br><img src='speaker.png' class='mini-speaker' width=20 height=20> Fireworks-like</div>
<div class='snd-cnt'><img class=snd src='0142.jpg' data-sound=0142.mp3 width=158 height=158><br><img src='speaker.png' class='mini-speaker' width=20 height=20> Underwater-like</div>
<div class='snd-cnt'><img class=snd src='0139.jpg' data-sound=0139.mp3 width=158 height=158><br><img src='speaker.png' class='mini-speaker' width=20 height=20> Car-like</div>
<div class='snd-cnt'><img class=snd src='0103.jpg' data-sound=0103.mp3 width=158 height=158><br><img src='speaker.png' class='mini-speaker' width=20 height=20> Parents-like</div>
<div class='snd-cnt'><img class=snd src='0051.jpg' data-sound=0051.mp3 width=158 height=158><br><img src='speaker.png' class='mini-speaker' width=20 height=20> Water-like</div>
<div class='snd-cnt'><img class=snd src='0745.jpg' data-sound=0745.mp3 width=158 height=158><br><img src='speaker.png' class='mini-speaker' width=20 height=20> Baby Talk-like</div>
<div class='snd-cnt'><img class=snd src='0912.jpg' data-sound=0912.mp3 width=158 height=158><br><img src='speaker.png' class='mini-speaker' width=20 height=20> Race Car-like</div>
<div class='snd-cnt'><img class=snd src='0028.jpg' data-sound=0028.mp3 width=158 height=158><br><img src='speaker.png' class='mini-speaker' width=20 height=20> Open Space-like</div>
<div class='snd-cnt'><img class=snd src='0411.jpg' data-sound=0411.mp3 width=158 height=158><br><img src='speaker.png' class='mini-speaker' width=20 height=20> Talking-like</div>
<div class='snd-cnt'><img class=snd src='0168.jpg' data-sound=0168.mp3 width=158 height=158><br><img src='speaker.png' class='mini-speaker' width=20 height=20> Cheering-like</div>

</div>

<h3>Visualizing conv5</h3>

<p>We can also visualize middle layers in the network. Interestingly, detectors for mid-level concepts automatically emerge in <tt>conv5</tt>.</p>

<div style='text-align:center;'>
<div class='snd-cnt snd-small'><img class="snd" src='speaker' data-sound=conv5/0050.mp3 width=64 height=64><br>Tapping-like</div>
<div class='snd-cnt snd-small'><img class="snd" src='speaker' data-sound=conv5/0060.mp3 width=64 height=64><br>Thumping-like</div>
<div class='snd-cnt snd-small'><img class="snd" src='speaker' data-sound=conv5/0073.mp3 width=64 height=64><br>Yelling-like</div>
<div class='snd-cnt snd-small'><img class="snd" src='speaker' data-sound=conv5/0094.mp3 width=64 height=64><br>Voice-like</div>
<div class='snd-cnt snd-small'><img class="snd" src='speaker' data-sound=conv5/0015.mp3 width=64 height=64><br>Swooshing-like</div>
<div class='snd-cnt snd-small'><img class="snd" src='speaker' data-sound=conv5/0117.mp3 width=64 height=64><br>Chiming-like</div>
<div class='snd-cnt snd-small'><img class="snd" src='speaker' data-sound=conv5/0144.mp3 width=64 height=64><br>Smacking-like</div>
<div class='snd-cnt snd-small'><img class="snd" src='speaker' data-sound=conv5/0146.mp3 width=64 height=64><br>Laughing-like</div>
<div class='snd-cnt snd-small'><img class="snd" src='speaker' data-sound=conv5/0166.mp3 width=64 height=64><br>Music Tune-like</div>
<div class='snd-cnt snd-small'><img class="snd" src='speaker' data-sound=conv5/0218.mp3 width=64 height=64><br>Clicking-like</div>
</div>

<h3>Visualizing conv1</h3>
<p>We visualize the first layer of the network by looking at the learned weights of <tt>conv1</tt>, which you can see below. The network operates on raw waveforms, so the filters are in the time-domain.</p>

<div style='text-align:center;'>
<div class='snd-cnt snd-tiny'><img class="snd" src='conv1/6.jpg' data-sound=conv1/6.mp3 width=72 height=50></div>
<div class='snd-cnt snd-tiny'><img class="snd" src='conv1/13.jpg' data-sound=conv1/13.mp3 width=72 height=50></div>
<div class='snd-cnt snd-tiny'><img class="snd" src='conv1/1.jpg' data-sound=conv1/1.mp3 width=72 height=50></div>
<div class='snd-cnt snd-tiny'><img class="snd" src='conv1/4.jpg' data-sound=conv1/4.mp3 width=72 height=50></div>
<div class='snd-cnt snd-tiny'><img class="snd" src='conv1/2.jpg' data-sound=conv1/2.mp3 width=72 height=50></div>
<div class='snd-cnt snd-tiny'><img class="snd" src='conv1/5.jpg' data-sound=conv1/5.mp3 width=72 height=50></div>
<div class='snd-cnt snd-tiny'><img class="snd" src='conv1/9.jpg' data-sound=conv1/9.mp3 width=72 height=50></div>
<div class='snd-cnt snd-tiny'><img class="snd" src='conv1/8.jpg' data-sound=conv1/8.mp3 width=72 height=50></div>
<div class='snd-cnt snd-tiny'><img class="snd" src='conv1/3.jpg' data-sound=conv1/3.mp3 width=72 height=50></div>
<div class='snd-cnt snd-tiny'><img class="snd" src='conv1/10.jpg' data-sound=conv1/10.mp3 width=72 height=50></div>
<div class='snd-cnt snd-tiny'><img class="snd" src='conv1/12.jpg' data-sound=conv1/12.mp3 width=72 height=50></div>
<div class='snd-cnt snd-tiny'><img class="snd" src='conv1/14.jpg' data-sound=conv1/14.mp3 width=72 height=50></div>
<div class='snd-cnt snd-tiny'><img class="snd" src='conv1/15.jpg' data-sound=conv1/15.mp3 width=72 height=50></div>
<div class='snd-cnt snd-tiny'><img class="snd" src='conv1/7.jpg' data-sound=conv1/7.mp3 width=72 height=50></div>
<div class='snd-cnt snd-tiny'><img class="snd" src='conv1/11.jpg' data-sound=conv1/11.mp3 width=72 height=50></div>
<div class='snd-cnt snd-tiny'><img class="snd" src='conv1/16.jpg' data-sound=conv1/16.mp3 width=72 height=50></div>
</div>

</div>




<div class="content">
<h2>Video Overview</h2>
<div style='text-align:center;'>
<iframe width="700" height="393" src="https://www.youtube.com/embed/yJCjVvIY4dU?rel=0&amp;showinfo=0" frameborder="0" allowfullscreenstyle='border-radius : 20px;'></iframe>
</div>
</div>


<div class="content">
<h2>Performance</h2>

<p>We experiment with SoundNet features on several tasks. They generally advance the state-of-the-art in environmental sound recognition by over 10%. By leveraging millions of unconstrained videos, we can learn better sound features.</p>

<table style='margin:0 auto;'>
<tr>
<td style='padding-right:20px;'><img src='dcase.png' height='140'></td>
<td><img src='esc.png' height='170'></td>
</tr>
<tr>
<th>DCASE</th>
<th>ESC 10 and ESC 50</th>
</tr>
</table>

<p>We also analyzed the performance of different components of our system. Our experiments suggest that one may obtain better performance
simply by downloading more videos, creating deeper networks, and leveraging richer vision models.</p>

<div style='text-align:center;'><img src='breakdown.png' width='500'></div>

<p>Check out the <a href="https://arxiv.org/pdf/1610.09001.pdf">paper</a> for full details and more analysis.</p>

</div>


<div class="content">
<h2>Code &amp; Trained Models</h2>
<a href="https://github.com/cvondrick/soundnet"><img src="https://assets-cdn.github.com/images/modules/logos_page/GitHub-Mark.png" style="float:right; width:100px; margin-right:15px; margin-top:-25px; margin-bottom : 10px;"></a>


<p>The code and models are available on Github and open source. It is implemented in Torch7. Using our pre-trained model, you can extract discriminative features for natural sound recognition. In our experiments, <tt>pool5</tt> seems to work the best with a linear SVM.</p>

<ul>
<li><a href="https://github.com/cvondrick/soundnet">Code on Github</a></li>
<li><a href="http://data.csail.mit.edu/soundnet/soundnet_models_public.zip">Pretrained Models</a> (101 MB zip file)</li>
</ul>

<p>Using the code is easy in Torch7:</p>
<p>
<pre>
sound = audio.load('file.mp3'):mul(2^-23):view(1,1,-1,1):cuda()
predictions = net:forward(sound)
</pre>
</p>

<p>SoundNet outputs two probability distributions of the categories that it recognizes for the input sounds. The first distribution are object categories, and the second distribution is scene categories. You can find the list of categories below:</p>
<ul>
<li><a href="http://data.csail.mit.edu/soundnet/categories/categories_imagenet.txt">Object Categories</a> (txt)</li>
<li><a href="http://data.csail.mit.edu/soundnet/categories/categories_places2.txt">Scene Categories</a> (txt)</li>
</ul>

<p>Minor Note: SoundNet was trained with an older version of Places365. While Places365 will give good results, if you want to strictly reproduce our results, please use this <a href="http://soundnet.csail.mit.edu/vgg16_places2/">VGG16 model</a> that has 401 categories instead.</p>

</div>


<div class="content" id="data">
<h2>Data</h2>

<p>Three-paired image datasets are considered for imitating the visual environment, from the complex background (CIFAR-10 and ImageNet) to the simple black background (COIL-20).</p>

<ul>
<li> Complex visual environment:
	<a href="https://www.cs.toronto.edu/~kriz/cifar.html">CIFAR-10</a>
    <a href="http://www.image-net.org/">ImageNet</a>
</li> 
<li> Simple handwritten digits:
	<a href="http://data.csail.mit.edu/soundnet/image_features_public.tar.gz">MNIST</a>
    <a href="https://www.nist.gov/itl/iad/image-group/emnist-dataset">EMNIST</a>
</li>
<li> Realistic SS training environment:
	<a href="http://www1.cs.columbia.edu/CAVE/software/softlib/coil-20.php">COIL-20</a>
</li>

</ul>

<p>The sound of the images in these datasets can be directedly generated by using the released encoding schemes above.</p>

</div>



<div class="content">
<h2>Reference</h2>

<p> Machine cross-modal perception is an exciting area of research but has just set sail! Check out some related work in Cognitive Science below:</p>

<ul>
<li><a href="https://www.seeingwithsound.com/">The vOICe Sensory Substitution Device</a><br></li>

<li><a href="https://content.iospress.com/articles/restorative-neurology-and-neuroscience/rnn160647">Designing sensory-substitution devices: Principles, pitfalls and potential</a><br>
by Kristja´nsson et al (CVPR 2016)</li>

<li><a href="https://www.nature.com/articles/srep15628">Auditory sensory substitution is intuitive and automatic with texture stimuli</a><br>
by Stiles et al</li>

<li><a href="https://www.sciencedirect.com/science/article/pii/S0149763412001200">Sensory substitution as an artificially acquired synaesthesia</a><br>
by Ward et al</li>

</ul>

</div>

<div class="content">
<h2>Acknowledgements</h2>

<p>We thank our lab colleagues for conducting the Sensory Substitution Tests. This work was supported in part by the National Natural Science Foundation of China grant under number 61772427, 61751202, U1864204 and 61773316, Natural Science Foundation of Shaanxi Province under Grant 2018KJXX-024, and Project of Special Zone for National Defense Science and Technology Innovation.
</div>

</body>
</html>
